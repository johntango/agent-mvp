# app/agents/design_from_story.py
from __future__ import annotations
import json, os
from pathlib import Path
from typing import Dict, Any
from openai import OpenAI
from app.config import load_config

def load_story_for_task(task_id: str, cfg: dict) -> dict:
    # Prefer the pointer generated by test generator
    ref = Path(cfg["LOCAL_GENERATED_ROOT"]) / task_id / "meta" / "story_ref.json"
    if ref.exists():
        obj = json.loads(ref.read_text(encoding="utf-8"))
        story_file = obj.get("story_file")
        if story_file:
            p = Path(cfg["LOCAL_STORY_ROOT"]) / story_file
            if p.exists():
                return json.loads(p.read_text(encoding="utf-8"))
    # Fallback: if exactly one story exists
    stories = sorted(Path(cfg["LOCAL_STORY_ROOT"]).glob("*.json"))
    if len(stories) == 1:
        return json.loads(stories[0].read_text(encoding="utf-8"))
    raise FileNotFoundError("No story available for design prompt")

def generate_design_from_story(task_id: str, *, model: str | None = None) -> dict:
    cfg = load_config()
    story = load_story_for_task(task_id, cfg)
    model = model or os.getenv("DESIGN_MODEL", "gpt-4o-mini")

    feature = story.get("feature","")
    requirements = story.get("requirements","")
    acceptance = story.get("acceptance","")
    constraints = story.get("constraints","")

    prompt = f"""You are a senior Python designer. Produce a precise plan for implementation.

Story Feature:
{feature}

Functional Requirements:
{requirements}

Acceptance Criteria:
{acceptance}

Constraints:
{constraints}

Output JSON with keys:
- design_summary: 1â€“3 paragraph summary referencing the story,
- files_touched: list of repo-relative file paths you plan to create/modify under src/,
- key_functions: list of function signatures,
- edge_cases: bullet list of edge cases derived from acceptance/constraints.
"""
    client = OpenAI()
    resp = client.chat.completions.create(
        model=model,
        messages=[{"role":"system","content":"Output ONLY JSON."},
                  {"role":"user","content": prompt}],
        temperature=0.2,
    )
    import json as _json
    text = (resp.choices[0].message.content or "").strip()
    try:
        obj = _json.loads(text)
    except Exception:
        # fallback minimal design if LLM didn't return JSON
        obj = {
            "design_summary": feature,
            "files_touched": ["src/main.py"],
            "key_functions": [],
            "edge_cases": [],
        }

    # Persist as design@v1.json for the pipeline
    data_dir = Path(cfg["APP_DATA_DIR"]) / task_id
    data_dir.mkdir(parents=True, exist_ok=True)
    # attach the raw story to the design artifact for traceability
    obj["_story"] = story
    (data_dir / "design@v1.json").write_text(json.dumps(obj, indent=2), encoding="utf-8")
    return obj
